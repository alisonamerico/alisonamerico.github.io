{"categories":[{"link":"/categories/testing/","name":"Testing","slug":"Testing"}],"pages":[],"posts":[{"link":"/posts/testing-github-apis-with-pytest-practice/","text":" Introduction In the previous post, we covered the theoretical foundations of Pytest.\nNow we will apply everything in practice using a real example that consumes the GitHub public API:\nBuild a small real-world project Consume the public GitHub API Apply unit tests Apply integration tests Use fixtures Use mocks Use markers Apply the AAA pattern Use pytest plugins This guide is intentionally very detailed. Every important line will be explained so that even beginners can follow.\nProject Structure TEXT Collapse Copy github_api_tests/ ‚îú‚îÄ‚îÄ app.py # Flask web application ‚îú‚îÄ‚îÄ services/ ‚îÇ ‚îî‚îÄ‚îÄ github_service.py # GitHub API service layer ‚îú‚îÄ‚îÄ tests/ ‚îÇ ‚îú‚îÄ‚îÄ conftest.py # Shared fixtures ‚îÇ ‚îú‚îÄ‚îÄ test_users_unit.py # Unit tests with mocks ‚îÇ ‚îú‚îÄ‚îÄ test_users_integration.py # Integration tests (real API) ‚îÇ ‚îú‚îÄ‚îÄ test_parametrize.py # Parameterized tests ‚îÇ ‚îú‚îÄ‚îÄ test_skip.py # Skip marker examples ‚îÇ ‚îú‚îÄ‚îÄ test_fail.py # xfail marker examples ‚îÇ ‚îú‚îÄ‚îÄ test_errors.py # Error handling tests ‚îÇ ‚îú‚îÄ‚îÄ test_functional.py # Functional tests (Flask routes) ‚îÇ ‚îú‚îÄ‚îÄ test_performace.py # Performance benchmarks ‚îÇ ‚îî‚îÄ‚îÄ test_regression.py # Regression tests ‚îú‚îÄ‚îÄ pytest.ini # Pytest configuration ‚îî‚îÄ‚îÄ requirements.txt # Project dependencies Click to expand and view more We organize the project following best practices:\napp.py - Main Flask application (entry point) services/ - Business logic layer (separates concerns) tests/ - All test files organized by type pytest.ini - Global test configuration requirements.txt - Dependency management This architecture provides:\nClear separation of concerns Easy maintenance and scalability Industry-standard structure Better test organization Project Setup Before running the tests, we need to prepare our local development environment properly.\nCreating a Virtual Environment A virtual environment allows us to isolate project dependencies from the global Python installation.\nThis prevents version conflicts between different projects and ensures reproducibility.\nBASH Collapse Copy python -m venv .venv Click to expand and view more What this command does: python ‚Üí Runs the Python interpreter.\n-m venv ‚Üí Executes the built-in venv module.\n.venv ‚Üí Creates a new virtual environment folder named .venv.\nAfter running this command, a directory called .venv/ will be created containing:\nA local Python interpreter\nIts own pip\nAn isolated site-packages directory\nThis ensures that any package installed inside this environment will not affect other projects.\nActivating the Virtual Environment Once created, the virtual environment must be activated:\nBASH Collapse Copy source .venv/bin/activate Click to expand and view more What this does: source ‚Üí Executes the script in the current shell.\n.venv/bin/activate ‚Üí Activates the virtual environment.\nAfter activation:\nYour terminal prompt usually changes (e.g., (.venv) appears).\npython and pip now point to the virtual environment versions.\nAll installed packages will be isolated inside .venv.\nDependencies All project dependencies are listed in a requirements.txt file:\nPLAINTEXT Collapse Copy flask requests pytest pytest-mock pytest-cov pytest-benchmark Click to expand and view more What each dependency does:\nflask ‚Üí Web framework for building the API endpoints. requests ‚Üí HTTP client used to interact with the GitHub API. pytest ‚Üí Testing framework. pytest-mock ‚Üí Provides integration between pytest and unittest.mock. pytest-cov ‚Üí Adds test coverage reporting. pytest-benchmark ‚Üí Performance testing and benchmarking tools. Installing Dependencies To install all required packages:\nBASH Collapse Copy pip install -r requirements.txt Click to expand and view more What this command does:\npip ‚Üí Python package installer. -r requirements.txt ‚Üí Reads the dependency list from the file. Installs all listed packages inside the active virtual environment. Pytest Configuration The project includes a pytest.ini file to configure pytest behavior globally.\npytest.ini\nINI Collapse Copy [pytest] addopts = -ra -q --cov=app --cov-report=term-missing markers = unit: marks tests as unit tests integration: marks tests as integration tests slow: marks tests as slow tests regression: marks tests as regression tests Click to expand and view more Line-by-line explanation [pytest]\nDeclares that this file contains configuration for pytest.\naddopts = -ra -q --cov=app --cov-report=term-missing\nDefines default command-line options that will always be applied when running pytest.\nBreaking it down:\n-r a ‚Üí Shows a summary for all test outcomes (passed, skipped, xfailed, etc.). -q ‚Üí Runs pytest in quiet mode (less verbose output). --cov=app ‚Üí Measures test coverage for the app package. --cov-report=term-missing ‚Üí Displays missing lines directly in the terminal coverage report. This ensures that every test run automatically includes coverage analysis.\nmarkers =\nRegisters custom test markers to avoid warnings like:\nMAKEFILE Collapse Copy PytestUnknownMarkWarning: Unknown pytest.mark.integration Click to expand and view more Each marker must be declared explicitly.\nunit\nINI Collapse Copy unit: marks tests as unit tests Click to expand and view more Marks tests as unit tests.\nThese tests:\nValidate small pieces of logic Avoid external dependencies Run very fast You can run them using:\nBASH Collapse Copy pytest -m unit Click to expand and view more integration\nINI Collapse Copy integration: marks tests as integration tests Click to expand and view more Marks integration tests.\nThese tests:\nValidate interaction between components May call real services or external APIs Are usually slower than unit tests Run only integration tests:\nBASH Collapse Copy pytest -m integration Click to expand and view more slow\nINI Collapse Copy slow: marks tests as slow tests Click to expand and view more Marks slower tests.\nYou can exclude them:\nBASH Collapse Copy pytest -m \u0026#34;not slow\u0026#34; Click to expand and view more Production Code Flask Application (app.py) PYTHON Collapse Copy # app.py from flask import Flask, jsonify from services.github_service import fetch_users app = Flask(__name__) @app.route(\u0026#39;/users\u0026#39;) def github_users(): try: users = fetch_users() return jsonify(users) except Exception as e: return jsonify({\u0026#34;error\u0026#34;: str(e)}), 500 if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True) Click to expand and view more Service Layer (services/github_service.py) PYTHON Collapse Copy # services/github_service.py import requests GITHUB_API_URL = \u0026#39;https://api.github.com/users\u0026#39; def fetch_users(per_page=10): \u0026#34;\u0026#34;\u0026#34; Fetches users from the public GitHub API. \u0026#34;\u0026#34;\u0026#34; response = requests.get( GITHUB_API_URL, params={\u0026#39;per_page\u0026#39;: per_page}, timeout=5 ) response.raise_for_status() return response.json() Click to expand and view more Production Code (Line-by-Line Explanation) Flask Application (app.py) PYTHON Collapse Copy # app.py from flask import Flask, jsonify from services.github_service import fetch_users Click to expand and view more We import:\nFlask - Web framework to create our API jsonify - Flask helper for JSON responses fetch_users - Service function that handles GitHub API calls PYTHON Collapse Copy app = Flask(__name__) Click to expand and view more Creates the Flask application instance.\nPYTHON Collapse Copy @app.route(\u0026#39;/users\u0026#39;) def github_users(): try: users = fetch_users() return jsonify(users) except Exception as e: return jsonify({\u0026#34;error\u0026#34;: str(e)}), 500 Click to expand and view more This is a Flask route with error handling:\n@app.route('/users') ‚Üí Maps HTTP GET /users to this function try/except ‚Üí Handles API failures gracefully jsonify(users) ‚Üí Returns the JSON response to the client return jsonify({\u0026quot;error\u0026quot;: str(e)}), 500 ‚Üí Returns error response when GitHub API fails PYTHON Collapse Copy if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True) Click to expand and view more Allows running the app directly with python app.py.\nService Layer (services/github_service.py) PYTHON Collapse Copy # services/github_service.py import requests GITHUB_API_URL = \u0026#39;https://api.github.com/users\u0026#39; Click to expand and view more Import requests for HTTP calls Define constant for GitHub API base URL PYTHON Collapse Copy def fetch_users(per_page=10): \u0026#34;\u0026#34;\u0026#34; Fetches users from the public GitHub API. \u0026#34;\u0026#34;\u0026#34; Fixture que cria um cliente HTTP de teste para o Flask. \u0026#34;\u0026#34;\u0026#34; with app.test_client() as client: yield client @pytest.fixture def sample_username(): return \u0026#39;octocat\u0026#39; Click to expand and view more This fixture provides reusable test data.\nFixtures (Explanation) PYTHON Collapse Copy # tests/conftest.py import pytest from app import app Click to expand and view more We import pytest and the Flask app.\nFlask Test Client Fixture PYTHON Collapse Copy @pytest.fixture def client(): \u0026#34;\u0026#34;\u0026#34; Fixture that creates HTTP test client for Flask. \u0026#34;\u0026#34;\u0026#34; with app.test_client() as client: yield client Click to expand and view more Line-by-line breakdown:\n@pytest.fixture ‚Üí Registers this function as a fixture client() ‚Üí Function name becomes injectable app.test_client() ‚Üí Creates Flask test client with ... as client: ‚Üí Context manager for cleanup yield client ‚Üí Provides client to tests, then cleans up This fixture allows testing Flask routes without starting a real server.\nSample Data Fixture PYTHON Collapse Copy @pytest.fixture def sample_username(): return \u0026#39;octocat\u0026#39; Click to expand and view more Line-by-line:\n@pytest.fixture ‚Üí Registers as fixture sample_username ‚Üí becomes injectable parameter return 'octocat' ‚Üí Provides test data When tests include these parameters:\nPYTHON Collapse Copy def test_example(client, sample_username): Click to expand and view more Pytest automatically injects both fixtures. This is dependency injection.\nUnit Test with Mock PYTHON Collapse Copy # tests/test_users_unit.py from services.github_service import fetch_users def test_fetch_users_with_pytest_mock(mocker): \u0026#34;\u0026#34;\u0026#34; Same unit test, using pytest-mock. \u0026#34;\u0026#34;\u0026#34; # Arrange fake_users = [{\u0026#39;login\u0026#39;: \u0026#39;pytest-mock\u0026#39;}] mocker.patch( \u0026#39;services.github_service.requests.get\u0026#39;, return_value=mocker.Mock( json=lambda: fake_users, raise_for_status=lambda: None ), ) # Act users = fetch_users() # Assert assert users == fake_users Click to expand and view more Unit Test (Line-by-Line Explanation) PYTHON Collapse Copy # tests/test_users_unit.py from services.github_service import fetch_users Click to expand and view more We import the service function to test it in isolation.\nPYTHON Collapse Copy def test_fetch_users_with_pytest_mock(mocker): Click to expand and view more mocker comes from pytest-mock plugin No explicit marker needed, but could use @pytest.mark.unit Arrange Section PYTHON Collapse Copy fake_users = [{\u0026#39;login\u0026#39;: \u0026#39;pytest-mock\u0026#39;}] Click to expand and view more Creates expected test data.\nPYTHON Collapse Copy mocker.patch( \u0026#39;services.github_service.requests.get\u0026#39;, return_value=mocker.Mock( json=lambda: fake_users, raise_for_status=lambda: None ), ) Click to expand and view more This is the mocking setup:\nmocker.patch() ‚Üí Replaces requests.get in our service return_value=mocker.Mock() ‚Üí Creates mock response json=lambda: fake_users ‚Üí Mock method that returns our fake data raise_for_status=lambda: None ‚Üí Mock that does nothing (no exception) Key point: No real HTTP call is made!\nAct Section PYTHON Collapse Copy users = fetch_users() Click to expand and view more Calls the service function, which uses the mocked requests.get.\nAssert Section PYTHON Collapse Copy assert users == fake_users Click to expand and view more Verifies the service returns exactly what our mock provided.\nIntegration Test (Real API Call) PYTHON Collapse Copy # tests/test_users_integration.py import pytest from services.github_service import fetch_users @pytest.mark.integration @pytest.mark.skip(reason=\u0026#34;GitHub API rate limit exceeded - skip for now\u0026#34;) def test_fetch_users_integration(): \u0026#34;\u0026#34;\u0026#34; Calls REAL GitHub API \u0026#34;\u0026#34;\u0026#34; # Act users = fetch_users(5) # Assert assert isinstance(users, list) assert len(users) \u0026gt; 0 assert \u0026#39;login\u0026#39; in users[0] Click to expand and view more This test makes a real HTTP call to GitHub.\nIntegration Test (Line-by-Line Explanation) PYTHON Collapse Copy @pytest.mark.integration def test_fetch_users_integration(): Click to expand and view more integration marker categorizes it as integration test No slow marker here, but could be added PYTHON Collapse Copy users = fetch_users(5) Click to expand and view more This performs a real request to GitHub API, asking for 5 users.\nPYTHON Collapse Copy assert isinstance(users, list) assert len(users) \u0026gt; 0 assert \u0026#39;login\u0026#39; in users[0] Click to expand and view more Multiple assertions validate:\nResponse is a list List is not empty First user has expected structure Important: This test requires internet connection and can be slow.\nParametrize PYTHON Collapse Copy # tests/test_parametrize.py import pytest from services.github_service import fetch_users @pytest.mark.parametrize(\u0026#39;qty\u0026#39;, [1, 3, 5]) def test_fetch_users_parametrize(qty, mocker): \u0026#34;\u0026#34;\u0026#34; Same test with multiple values. Using mocked requests to avoid rate limiting. \u0026#34;\u0026#34;\u0026#34; # Arrange - mock response to avoid rate limiting fake_users = [{\u0026#39;login\u0026#39;: f\u0026#39;user{i}\u0026#39;} for i in range(qty)] mocker.patch( \u0026#39;services.github_service.requests.get\u0026#39;, return_value=mocker.Mock( json=lambda: fake_users, raise_for_status=lambda: None ), ) # Act users = fetch_users(qty) # Assert assert len(users) \u0026lt;= qty assert len(users) == qty # Should match exactly with our mock Click to expand and view more Pytest runs this test three times with different quantities.\nParametrize (Detailed) PYTHON Collapse Copy @pytest.mark.parametrize(\u0026#39;qty\u0026#39;, [1, 3, 5]) Click to expand and view more Pytest will:\nRun the test with qty = 1 Run it with qty = 3 Run it with qty = 5 PYTHON Collapse Copy def test_fetch_users_parametrize(qty, mocker): fake_users = [{\u0026#39;login\u0026#39;: f\u0026#39;user{i}\u0026#39;} for i in range(qty)] mocker.patch(\u0026#39;services.github_service.requests.get\u0026#39;, ...) users = fetch_users(qty) assert len(users) == qty Click to expand and view more The parameters are automatically injected into the test function.\nThis approach:\nEliminates code duplication Tests edge cases (1, 3, 5 users) Uses mocking to avoid API rate limiting Makes tests more maintainable Pro tip: You can also combine multiple parameters:\nPYTHON Collapse Copy @pytest.mark.parametrize(\u0026#39;qty,expected\u0026#39;, [(1, 1), (5, 5), (100, 100)]) Click to expand and view more Testing Errors PYTHON Collapse Copy # tests/test_errors.py import pytest import requests from services.github_service import fetch_users def test_timeout_error(mocker): \u0026#34;\u0026#34;\u0026#34; Tests how service handles timeouts. \u0026#34;\u0026#34;\u0026#34; # Arrange mocker.patch( \u0026#39;services.github_service.requests.get\u0026#39;, side_effect=requests.Timeout(\u0026#34;Request timed out\u0026#34;) ) # Act \u0026amp; Assert with pytest.raises(requests.Timeout): fetch_users() def test_http_error_handling(mocker): \u0026#34;\u0026#34;\u0026#34; Tests HTTP error handling. \u0026#34;\u0026#34;\u0026#34; # Arrange mocker.patch( \u0026#39;services.github_service.requests.get\u0026#39;, side_effect=requests.HTTPError(\u0026#34;404 Not Found\u0026#34;) ) # Act \u0026amp; Assert with pytest.raises(requests.HTTPError): fetch_users() Click to expand and view more This ensures errors are handled correctly.\nTesting Errors (Line-by-Line Explanation) Timeout Error Test PYTHON Collapse Copy mocker.patch( \u0026#39;services.github_service.requests.get\u0026#39;, side_effect=requests.Timeout(\u0026#34;Request timed out\u0026#34;) ) Click to expand and view more side_effect ‚Üí Instead of returning, raises exception requests.Timeout ‚Üí Specific timeout exception Tests how service handles network timeouts PYTHON Collapse Copy with pytest.raises(requests.Timeout): fetch_users() Click to expand and view more Asserts that fetch_users() raises Timeout Test passes if correct exception is raised Test fails if no exception or wrong exception HTTP Error Test PYTHON Collapse Copy side_effect=requests.HTTPError(\u0026#34;404 Not Found\u0026#34;) Click to expand and view more Simulates HTTP errors like 404, 500, etc.\nKey benefits:\nTests error handling without real failures Ensures proper exception propagation Verifies service robustness Skip vs Xfail PYTHON Collapse Copy # tests/test_skip.py import pytest @pytest.mark.skip(reason=\u0026#39;Feature under development\u0026#39;) def test_future_feature(): assert True Click to expand and view more PYTHON Collapse Copy # tests/test_fail.py import pytest from services.github_service import fetch_users @pytest.mark.xfail(reason=\u0026#39;Known bug\u0026#39;) def test_known_bug(): assert False @pytest.mark.xfail(reason=\u0026#39;Known bug when per_page \u0026gt; 100\u0026#39;) def test_expected_failure(): fetch_users(200) Click to expand and view more Skip vs Xfail (Detailed) Skip Marker PYTHON Collapse Copy @pytest.mark.skip(reason=\u0026#39;Feature under development\u0026#39;) Click to expand and view more Test does not run at all Shows as \u0026ldquo;skipped\u0026rdquo; in test report Useful for incomplete features Saves execution time Xfail Marker PYTHON Collapse Copy @pytest.mark.xfail(reason=\u0026#39;Known bug\u0026#39;) Click to expand and view more Test runs but failure is expected Shows as \u0026ldquo;xfail\u0026rdquo; (expected failure) if it fails Shows as \u0026ldquo;xpass\u0026rdquo; (unexpected pass) if it passes Useful for known bugs or edge cases When to use each:\n@pytest.mark.skip: Feature not ready, environment issues @pytest.mark.xfail: Known limitations, expected failures Conditional skipping:\nPYTHON Collapse Copy @pytest.mark.skipif(sys.version_info \u0026lt; (3, 8), reason=\u0026#34;Requires Python 3.8+\u0026#34;) def test_python_38_feature(): pass Click to expand and view more Functional Tests (Flask Routes) PYTHON Collapse Copy # tests/test_functional.py from http import HTTPStatus def test_users_page(client): \u0026#34;\u0026#34;\u0026#34; Simulates access to Flask route \u0026#34;\u0026#34;\u0026#34; # Act response = client.get(\u0026#39;/users\u0026#39;) # Assert - check for either successful response or rate limit error assert response.status_code in [HTTPStatus.OK, HTTPStatus.INTERNAL_SERVER_ERROR] # Check if response is valid JSON try: data = json.loads(response.data) if response.status_code == HTTPStatus.OK: assert isinstance(data, list) else: # Should be an error response assert isinstance(data, dict) assert \u0026#39;error\u0026#39; in data except json.JSONDecodeError: assert False, \u0026#34;Response is not valid JSON\u0026#34; Click to expand and view more Functional Tests (Line-by-Line Explanation) PYTHON Collapse Copy def test_users_page(client): Click to expand and view more client is the Flask test client fixture Tests the web route end-to-end Handles both success and error scenarios PYTHON Collapse Copy response = client.get(\u0026#39;/users\u0026#39;) Click to expand and view more Simulates HTTP GET request to /users No real server needed (test client) PYTHON Collapse Copy assert response.status_code in [HTTPStatus.OK, HTTPStatus.INTERNAL_SERVER_ERROR] Click to expand and view more Accepts both success (200) and error (500) responses 500 occurs when GitHub API rate limits the request PYTHON Collapse Copy data = json.loads(response.data) if response.status_code == HTTPStatus.OK: assert isinstance(data, list) else: assert isinstance(data, dict) assert \u0026#39;error\u0026#39; in data Click to expand and view more Validates that response is always valid JSON Success: should be a list of users Error: should be a dict with \u0026rsquo;error\u0026rsquo; key Performance Tests PYTHON Collapse Copy # tests/test_performace.py def test_users_endpoint_performance(benchmark, client): \u0026#34;\u0026#34;\u0026#34; Measures response time of /users route. \u0026#34;\u0026#34;\u0026#34; benchmark(lambda: client.get(\u0026#39;/users\u0026#39;)) Click to expand and view more Performance Testing (Line-by-Line Explanation) PYTHON Collapse Copy def test_users_endpoint_performance(benchmark, client): Click to expand and view more benchmark fixture from pytest-benchmark client fixture for Flask testing PYTHON Collapse Copy benchmark(lambda: client.get(\u0026#39;/users\u0026#39;)) Click to expand and view more Measures execution time of the lambda Provides detailed performance metrics Establishes baseline for future comparisons Current benchmark results from the project:\nPLAINTEXT Collapse Copy ------------------------------------------------------- benchmark: 1 tests ------------------------------------------------------ Name (time in ms) Min Max Mean StdDev Median IQR Outliers OPS Rounds Iterations --------------------------------------------------------------------------------------------------------------------------------- test_users_endpoint_performance 246.0832 264.0037 254.0807 8.1997 252.4519 15.2874 1;0 3.9358 5 1 --------------------------------------------------------------------------------------------------------------------------------- Legend: Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile. OPS: Operations Per Second, computed as 1 / Mean Click to expand and view more Benchmark metrics explained:\nMin: Fastest execution time observed Max: Slowest execution time observed Mean: Average execution time across all rounds StdDev: Standard deviation (measure of consistency) Median: Middle value when sorted (less affected by outliers) IQR: Interquartile range (middle 50% of values) Outliers: Values that deviate significantly from the norm OPS: Operations per second (how many times it could run in one second) Rounds: Number of test iterations performed Iterations: Number of executions per round Run with: pytest --benchmark-only\nRegression Tests PYTHON Collapse Copy # tests/test_regression.py from http import HTTPStatus import pytest @pytest.mark.regression def test_users_endpoint_handles_gracefully(client): \u0026#34;\u0026#34;\u0026#34; Test that route handles errors gracefully. Even if external API fails, route should not crash. \u0026#34;\u0026#34;\u0026#34; # Act response = client.get(\u0026#39;/users\u0026#39;) # Should return proper JSON response even when GitHub API fails assert response.status_code in [HTTPStatus.OK, HTTPStatus.INTERNAL_SERVER_ERROR] # Response should always be valid JSON import json try: data = json.loads(response.data) assert isinstance(data, (list, dict)) except json.JSONDecodeError: assert False, \u0026#34;Response is not valid JSON\u0026#34; Click to expand and view more Regression Testing (Line-by-Line Explanation) PYTHON Collapse Copy @pytest.mark.regression Click to expand and view more Custom marker for regression tests Can be run with: pytest -m regression PYTHON Collapse Copy assert response.status_code in [HTTPStatus.OK, HTTPStatus.INTERNAL_SERVER_ERROR] Click to expand and view more Ensures the endpoint always returns proper responses Even when external API fails, returns structured JSON Prevents crashes and unhandled exceptions Run regression tests only: pytest -m regression\nCoverage BASH Collapse Copy pytest --cov=app --cov-report=term-missing Click to expand and view more This:\nTracks executed lines Shows missing lines in coverage report Integrated into pytest.ini for automatic coverage Current coverage report from the project:\nPLAINTEXT Collapse Copy ==================== tests coverage ==================== Name Stmts Miss Cover Missing -------------------------------------- app.py 12 2 83% 10, 15 -------------------------------------- TOTAL 12 2 83% Click to expand and view more Coverage metrics explained:\nName: File name being measured Stmts: Total statements (lines of code) in the file Miss: Number of statements not executed by tests Cover: Percentage of code covered by tests Missing: Specific line numbers not covered ‚ö†Ô∏è Important: High coverage does not mean high-quality tests. Focus on testing behavior, not just lines.\nRunning Different Test Types BASH Collapse Copy # Run only unit tests pytest -m unit # Run only integration tests pytest -m integration # Run regression tests pytest -m regression # Skip slow tests pytest -m \u0026#34;not slow\u0026#34; # Run performance benchmarks pytest --benchmark-only # Run with coverage (configured in pytest.ini) pytest # Run with verbose output pytest -v # Run specific test file pytest tests/test_users_unit.py Click to expand and view more Best Practices Summary Test Organization\nSeparate production code from tests Use descriptive test names Group related tests in files AAA Pattern\nArrange: Prepare test data and mocks Act: Execute the function under test Assert: Verify the outcome Mocking Strategy\nMock external dependencies in unit tests Use real calls in integration tests Mock only what you need (specific methods) Fixtures Usage\nReuse common test setup Keep fixtures focused and simple Use yield for cleanup operations Test Categories\nUnit: Fast, isolated, business logic Integration: Real external calls Functional: Full user scenarios Performance: Benchmark critical paths Regression: Prevent bug recurrence Advanced Tips Custom Markers PYTHON Collapse Copy # pytest.ini markers = unit: marks tests as unit tests integration: marks tests as integration tests regression: marks tests as regression tests slow: marks tests as slow tests network: marks tests requiring internet Click to expand and view more Test Configuration PYTHON Collapse Copy # conftest.py @pytest.fixture(scope=\u0026#34;session\u0026#34;) def api_client(): \u0026#34;\u0026#34;\u0026#34;Client shared across all tests\u0026#34;\u0026#34;\u0026#34; return SomeApiClient() @pytest.fixture(autouse=True) def setup_test_environment(): \u0026#34;\u0026#34;\u0026#34;Auto-used fixture for all tests\u0026#34;\u0026#34;\u0026#34; # Setup code here yield # Cleanup code here Click to expand and view more Conclusion You now understand:\nTest Organization: Proper structure and separation Unit Tests: Isolated testing with mocks Integration Tests: Real API calls Functional Tests: End-to-end scenarios Performance Tests: Benchmarking with pytest-benchmark Regression Tests: Preventing bug recurrence Mocking Strategy: When and how to mock Fixtures: Reusable test components Markers: Categorizing and filtering tests Parametrize: Reducing test duplication AAA Pattern: Clear test structure Coverage: Measuring test completeness This is professional-level test architecture that will scale with your project.\nFull project on Github ","title":"Testing GitHub APIs with Pytest - Practice"},{"link":"/posts/testing-with-pytest-fundamentals-best-practices-and-strategy/","text":" Testing software is not just a technical development step, it\u0026rsquo;s a quality, confidence, and sustainability strategy. Systems without tests tend to break frequently, create fear of changes, and drastically increase maintenance costs.\nIn this post, we\u0026rsquo;ll focus 100% on theory, explaining fundamental concepts of automated testing using Pytest, the most popular testing framework in the Python ecosystem.\nüëâ In the next post of the series, we\u0026rsquo;ll apply all of this to a real Flask project consuming the GitHub public API.\nWhat is Pytest? Pytest is a testing framework for Python that makes it easy to create simple, readable, and scalable tests. It allows testing from isolated functions to complete systems.\nWhy is Pytest so widely used? Simple syntax (based on assert) Automatic test discovery Powerful fixtures system Excellent plugin support Scales well for large projects Compared to unittest, Pytest is more expressive, less verbose, and more productive.\nWhat is the purpose of tests? Tests exist to reduce uncertainty.\nThey help to:\nEnsure code works as expected Detect errors quickly Prevent regressions Enable safe refactoring Serve as living documentation of the system Code without tests might work today, but it\u0026rsquo;s fragile tomorrow.\nWhen to write tests? When creating new features When fixing bugs (especially regressions) Before refactoring existing code In business-critical code And when NOT to write tests? Very quick proofs of concept (POCs) Completely disposable scripts Even in these cases, tests can still bring benefits.\nTypes of Tests Understanding test types is essential to create a balanced strategy.\nUnit Tests Test isolated functions or methods Don\u0026rsquo;t access database, network, or file system Are fast and cheap ‚úîÔ∏è Foundation of the test pyramid\nIntegration Tests Test communication between components Example: application + database, application + external API ‚úîÔ∏è Detect real integration failures ‚ùå Slower\nFunctional Tests Test complete application flows Simulate user behavior ‚úîÔ∏è High confidence ‚ùå More complex\nSystem / End-to-End (E2E) Tests Test the system as a whole Include multiple layers ‚úîÔ∏è Simulate real usage ‚ùå Slow and fragile\nRegression Tests Ensure fixed bugs don\u0026rsquo;t come back Usually arise after real incidents ‚úîÔ∏è Protect against recurring errors\nPerformance Tests Evaluate response time and resource consumption Identify bottlenecks ‚úîÔ∏è Important in critical systems ‚ùå Should be used with discretion\nThe Test Pyramid A good testing strategy follows the pyramid:\nMany unit tests Some integration tests Few E2E tests This ensures speed, confidence, and controlled cost.\nThe AAA Pattern (Arrange, Act, Assert) AAA is a test organization pattern that dramatically improves readability.\nArrange Prepare data, mocks, and context.\nAct Execute the action being tested.\nAssert Validate the result.\nBenefits Clearer tests Less ambiguity Easier maintenance Fixtures in Pytest Fixtures are reusable functions responsible for preparing and cleaning up the test environment.\nWhy use fixtures? Avoid code duplication Centralize setup/teardown Make tests more readable Fixture scopes function: default (runs for each test) module: once per file session: once per execution Pytest Marks Marks allow you to classify and control tests.\nparametrize Allows running the same test with multiple data sets.\nBenefits:\nLess code More coverage More expressive tests skip Consciously ignores tests.\nWhen to use:\nUnavailable external dependency Disabled functionality xfail Marks tests that should fail.\nBenefits:\nDocuments known bugs Doesn\u0026rsquo;t break the pipeline slow Marks slow tests.\nBenefits:\nSelective execution Quick feedback in daily work What is Mock? Mock is a technique used to simulate external dependencies.\nWhen to use mock? External APIs Database Third-party services When NOT to use? To test internal logic Excessively (overmocking) unittest.mock vs pytest-mock unittest.mock Python standard library More verbose Uses context managers ‚úîÔ∏è Doesn\u0026rsquo;t depend on plugins ‚ùå Code harder to read\npytest-mock Pytest plugin Integration with fixtures Cleaner syntax ‚úîÔ∏è More productive ‚úîÔ∏è More readable\nEssential Pytest Plugins pytest-cov Measures code coverage Helps identify untested areas ‚ö†Ô∏è High coverage ‚â† quality code\npytest-mock Facilitates mock creation Reduces boilerplate General Best Practices Tests should be simple One behavior per test Clear and descriptive names Avoid complex logic in tests Tests should be deterministic Conclusion Automated tests are not a luxury, they are a professional necessity. Pytest provides the right tools to write clear, scalable, and reliable tests.\nIn the next post of the series, we\u0026rsquo;ll apply all these concepts in a real Flask project, consuming the GitHub public API, with unit, integration, functional, and performance tests.\nüëâ Continue to Testing GitHub APIs with Pytest - Practice\n","title":"Testing with Pytest - Fundamentals, Best Practices and Strategy"},{"link":"/posts/understanding-mock-and-magicmock-in-python/","text":" This article was written so anyone can understand it (if there is anything unclear, let me know so it can be improved), even if you are just getting started with testing in Python.\nWe will explain what each concept is, why it exists, and when to use it, always with practical examples and clear explanations.\n1. What is a Mock? (simple explanation) A Mock is a fake object, created only for tests, that pretends to be a real object.\nIt allows you to:\nSimulate functions, methods, classes, and entire objects Define return values and exceptions Record calls (count, arguments, order) Isolate the unit of code under test Instead of using:\na real database, an external API, an email service, or a complex system, we use a mock to simulate this behavior.\nWhy is this important? Because in tests we want:\nspeed, predictability, isolation. Mental example Imagine a function that sends emails:\nPYTHON Collapse Copy def send_email(to): print(\u0026#34;Sending email\u0026#34;) Click to expand and view more In a test, you do not want to send real emails. So you replace this function with a mock.\nThe mock:\nsends nothing, but records whether it was called, with which arguments, and how many times. üëâ That is why we say a mock is flexible: you define how it should behave.\nüëâ Any attribute accessed on a Mock automatically becomes another Mock.\n2. What is MagicMock? MagicMock is a special type and subclass of Mock.\nIt exists to simulate objects that use Python magic methods.\nWhat are magic methods? They are methods that:\nstart and end with __ are called automatically by Python Common examples:\n__len__ ‚Üí called when using len(obj) __iter__ ‚Üí used in for loops __getitem__ ‚Üí used with obj[x] __contains__ ‚Üí used with in __str__ ‚Üí used with print(obj) These methods are not called directly, but by Python itself.\nWhy does MagicMock exist? A regular Mock does not handle these methods well. MagicMock is already prepared for them.\n3. Mock ‚â† Stub ‚â† Fake (important differences) Stub A stub only returns fixed values.\nPYTHON Collapse Copy def get_user_stub(): return {\u0026#34;id\u0026#34;: 1} Click to expand and view more Fake A fake has a simple but functional implementation.\nPYTHON Collapse Copy class FakeEmailService: def __init__(self): self.sent = [] def send(self, to): self.sent.append(to) Click to expand and view more Mock A mock records calls and validates interactions.\nPYTHON Collapse Copy from unittest.mock import Mock email = Mock() email.send(\u0026#34;a@test.com\u0026#34;) email.send.assert_called_once_with(\u0026#34;a@test.com\u0026#34;) Click to expand and view more 4. Main attributes and methods return_value Defines the value returned by the mock.\nPYTHON Collapse Copy mock.func.return_value = 10 Click to expand and view more side_effect Allows exceptions, functions, or multiple return values.\nPYTHON Collapse Copy mock.func.side_effect = Exception(\u0026#34;Error\u0026#34;) Click to expand and view more called / call_count Indicate whether and how many times it was called.\nPYTHON Collapse Copy assert mock.func.called assert mock.func.call_count == 1 Click to expand and view more call_args / call_args_list Show the arguments used.\nPYTHON Collapse Copy mock.func(1) mock.func(2) assert mock.func.call_args_list == [((1,),), ((2,),)] Click to expand and view more 5. patch ‚Äî replacing dependencies Use patch to temporarily replace real dependencies with mocks.\nGolden rule:\nYou must patch where the object is USED, not where it is DEFINED.\nPYTHON Collapse Copy # app/services.py from app.email import send_email def notify(user): send_email(user.email) Click to expand and view more PYTHON Collapse Copy with patch(\u0026#34;app.services.send_email\u0026#34;) as mock_send: notify(user) Click to expand and view more ‚ùå WRONG:\nPYTHON Collapse Copy patch(\u0026#34;app.email.send_email\u0026#34;) Click to expand and view more 6. spec and autospec Use them to avoid silent errors and ensure correct signatures.\nspec Restricts valid attributes.\nPYTHON Collapse Copy mock = Mock(spec=MyClass) Click to expand and view more autospec Restricts attributes and the function signature.\nPYTHON Collapse Copy @patch(\u0026#34;module.func\u0026#34;, autospec=True) def test(mock_func): ... Click to expand and view more ‚úÖ Prevents silent errors.\n7. Mock with logic ‚Üí use Fake If a mock contains complex logic, turn it into a fake.\n8. Common problems and how to solve them ‚ùå Mock does not work ‚û°Ô∏è Patch applied in the wrong place\n‚ùå Test passes but breaks in production ‚û°Ô∏è Missing autospec\n‚ùå Test is hard to understand ‚û°Ô∏è Too many mocks\n‚ùå Mock with logic ‚û°Ô∏è Extract into a fake\n9. When to use (and when NOT to use) Use mocks when: There are HTTP calls There is database access There is file read/write There are external services There are hard-to-reproduce dependencies Do NOT use mocks when: You are testing pure logic The test becomes a copy of the implementation The mock starts having its own logic Practical rule: mock dependencies, not business rules.\n10. Real best practices Use autospec=True Mock less, test more behavior Name your mocks well One test, one scenario 11. Anti-patterns üö´ Mocking private methods\nüö´ Mocking domain logic\nüö´ Testing only calls\nüö´ Excessive nested mocks\n12. Mental checklist Before creating a mock:\nIs this an external dependency? Does this test really need it? Would a fake be simpler? Am I testing behavior or implementation? References https://docs.python.org/3/library/unittest.mock.html ","title":"Understanding Mock and MagicMock in Python"}],"tags":[{"link":"/tags/flask/","name":"Flask","slug":"Flask"},{"link":"/tags/github/","name":"Github","slug":"Github"},{"link":"/tags/mock/","name":"Mock","slug":"Mock"},{"link":"/tags/pytest/","name":"Pytest","slug":"Pytest"},{"link":"/tags/python/","name":"Python","slug":"Python"},{"link":"/tags/tests/","name":"Tests","slug":"Tests"},{"link":"/tags/unittest/","name":"Unittest","slug":"Unittest"}]}